{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logging on proactive-server...\")\n",
    "proactive_host = 'try.activeeon.com'\n",
    "proactive_port = '8443'\n",
    "proactive_url  = \"https://\"+proactive_host+\":\"+proactive_port\n",
    "print(\"Creating gateway \")\n",
    "gateway = proactive.ProActiveGateway(proactive_url, debug=False)\n",
    "print(\"Gateway created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting on: \" + proactive_url)\n",
    "gateway.connect()\n",
    "assert gateway.isConnected() is True\n",
    "print(\"Connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating a proactive job...\")\n",
    "job = gateway.createJob(\"PythonMLWorkflow\")\n",
    "print(\"Job created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a fork environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding a fork environment to the import task...\")\n",
    "fork_env = gateway.createForkEnvironment(language=\"groovy\")\n",
    "fork_env.setImplementationFromFile(\"../scripts/fork_env.groovy\")\n",
    "job.addVariable(\"CONTAINER_PLATFORM\", \"docker\")\n",
    "job.addVariable(\"CONTAINER_IMAGE\", \"docker://activeeon/dlm3\")\n",
    "job.addVariable(\"CONTAINER_GPU_ENABLED\", \"false\")\n",
    "job.addVariable(\"CONTAINER_MOUNT_PATH\", \"/shared\")\n",
    "job.addVariable(\"HOST_MOUNT_PATH\", \"/shared\")\n",
    "print(\"Fork environment created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating task _import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the initialization task...\")\n",
    "init_task = gateway.createPythonTask(\"init\")\n",
    "init_task.setTaskImplementation(\"\"\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "boston = load_boston()\n",
    "dataframe_load = pd.DataFrame(boston.data)\n",
    "dataframe_load.columns = boston.feature_names \n",
    "data_label = boston.target\n",
    "dataframe = dataframe_load.assign(LABEL=data_label)\n",
    "\n",
    "dataframe_json = dataframe.to_json(orient='split').encode()\n",
    "compressed_data = bz2.compress(dataframe_json)\n",
    "dataframe.head()\n",
    "\n",
    "variables.put(\"dataframe_json\", dataframe_json)\"\"\")\n",
    "init_task.setForkEnvironment(fork_env)\n",
    "print(\"Task created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating task _split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the initialization task...\")\n",
    "split_task = gateway.createPythonTask(\"split\")\n",
    "split_task.setTaskImplementation(\"\"\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dataframe_json = variables.get(\"dataframe_json\")\n",
    "\n",
    "dataframe = pd.read_json(dataframe_json, orient='split')\n",
    "\n",
    "X_train, X_test = train_test_split(dataframe, test_size=30)\n",
    "\n",
    "X_train_json = X_train.to_json(orient='split').encode()\n",
    "X_test_json = X_test.to_json(orient='split').encode()\n",
    "\n",
    "variables.put(\"X_train_json\", X_train_json)\n",
    "variables.put(\"X_test_json\", X_test_json)\"\"\")\n",
    "split_task.setForkEnvironment(fork_env)\n",
    "split_task.addDependency(init_task)\n",
    "print(\"Task created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating task _train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the initialization task...\")\n",
    "train_task = gateway.createPythonTask(\"train\")\n",
    "train_task.setTaskImplementation(\"\"\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train_json = variables.get(\"X_train_json\")\n",
    "\n",
    "# Fit the model on 33%\n",
    "columns=\"LABEL\"\n",
    "X_train = pd.read_json(X_train_json, orient='split')\n",
    "dataframe_train = X_train.drop(columns, axis=1, inplace=False)\n",
    "dataframe_label = X_train.filter(columns, axis=1)\n",
    "model = LinearRegression()\n",
    "model.fit(dataframe_train, dataframe_label)\n",
    "# save the model to disk\n",
    "filename = '/shared/finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "variables.put(\"filename\", filename)\n",
    "variables.put(\"columns\", columns)\"\"\")\n",
    "train_task.setForkEnvironment(fork_env)\n",
    "train_task.addDependency(split_task)\n",
    "print(\"Task created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating task _predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the initialization task...\")\n",
    "predict_task = gateway.createPythonTask(\"predict\")\n",
    "predict_task.setTaskImplementation(\"\"\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import bz2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "columns = variables.get(\"columns\")\n",
    "filename = variables.get(\"filename\")\n",
    "X_test_json = variables.get(\"X_test_json\")\n",
    "\n",
    "X_test = pd.read_json(X_test_json, orient='split')\n",
    "\n",
    "dataframe_test = X_test.drop(columns, axis=1, inplace=False)\n",
    "dataframe_label = X_test.filter(columns, axis=1)\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(dataframe_test, dataframe_label)\n",
    "\n",
    "print('The prediction result is = ' + str(result))\"\"\")\n",
    "predict_task.setForkEnvironment(fork_env)\n",
    "predict_task.addDependency(split_task)\n",
    "predict_task.addDependency(train_task)\n",
    "predict_task.setPreciousResult(True)\n",
    "print(\"Task created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding tasks to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding tasks to the job...\")\n",
    "job.addTask(init_task)\n",
    "job.addTask(split_task)\n",
    "job.addTask(train_task)\n",
    "job.addTask(predict_task)\n",
    "print(\"Tasks added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the job to the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting the job to the proactive scheduler...\")\n",
    "job_id = gateway.submitJob(job, debug=False)\n",
    "print(\"job_id: \" + str(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting job status...\")\n",
    "job_status = gateway.getJobStatus(job_id)\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting job results and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting job results...\")\n",
    "job_result = gateway.getJobResult(job_id)\n",
    "print(job_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting job outputs...\")\n",
    "job_outputs = gateway.getJobOutput(job_id)\n",
    "print(job_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disconnecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Disconnecting\")\n",
    "gateway.close()\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
